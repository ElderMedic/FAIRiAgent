"""
Gradio Web UI for FAIRifier - Production Version
å®Œæ•´çš„ Gradio ç•Œé¢ï¼Œé›†æˆæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import gradio as gr
import asyncio
import os
import json
from pathlib import Path
from datetime import datetime
import logging
from typing import Optional, Dict, Tuple
import sys

# Add parent directories to path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from fairifier.graph.langgraph_app import FAIRifierLangGraphApp
from fairifier.config import config, apply_env_overrides

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global workflow instance (cached)
_workflow_instance = None

# Global log buffer for Gradio output
_gradio_log_buffer = []
_gradio_log_lock = asyncio.Lock()


class GradioLogHandler(logging.Handler):
    """Custom logging handler that captures logs for Gradio display."""
    def __init__(self):
        super().__init__()
        self.log_buffer = []
        self.max_buffer_size = 500  # Keep last 500 lines
    
    def emit(self, record):
        """Emit a log record to buffer."""
        try:
            msg = self.format(record)
            self.log_buffer.append(msg)
            # Keep only last N lines
            if len(self.log_buffer) > self.max_buffer_size:
                self.log_buffer = self.log_buffer[-self.max_buffer_size:]
            # Also add to global buffer
            global _gradio_log_buffer
            _gradio_log_buffer.append(msg)
            if len(_gradio_log_buffer) > self.max_buffer_size:
                _gradio_log_buffer = _gradio_log_buffer[-self.max_buffer_size:]
        except Exception:
            self.handleError(record)
    
    def get_logs(self):
        """Get all logs as string."""
        return '\n'.join(self.log_buffer)
    
    def clear(self):
        """Clear log buffer."""
        self.log_buffer = []
        global _gradio_log_buffer
        _gradio_log_buffer = []


def get_workflow():
    """Get cached workflow instance."""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = FAIRifierLangGraphApp()
    return _workflow_instance


def create_temp_env(
    llm_provider: str,
    llm_model: str,
    llm_temp: float,
    llm_tokens: int,
    mineru_enabled: bool,
    langsmith_key: str,
    project_id: str
) -> Path:
    """
    åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œè¦†ç›–é»˜è®¤é…ç½®
    """
    temp_env_path = config.output_path / project_id / ".env"
    temp_env_path.parent.mkdir(parents=True, exist_ok=True)

    env_content = f"""# Temporary configuration for project {project_id}
# Generated by Gradio UI

# LLM Configuration
LLM_PROVIDER={llm_provider}
LLM_MODEL={llm_model}
LLM_TEMPERATURE={llm_temp}
LLM_MAX_TOKENS={int(llm_tokens)}

# MinerU Configuration
MINERU_ENABLED={'true' if mineru_enabled else 'false'}

# LangSmith Configuration
"""
    if langsmith_key:
        env_content += f"LANGSMITH_API_KEY={langsmith_key}\n"

    temp_env_path.write_text(env_content)
    logger.info(f"ğŸ“ Created temp config: {temp_env_path}")

    return temp_env_path


def apply_user_config(
    llm_provider: str,
    llm_model: str,
    llm_temp: float,
    llm_tokens: int,
    mineru_enabled: bool,
    langsmith_key: str
):
    """
    åŠ¨æ€åº”ç”¨ç”¨æˆ·é…ç½®åˆ°å…¨å±€ config
    """
    # æ›´æ–°å…¨å±€é…ç½®
    config.llm_provider = llm_provider
    config.llm_model = llm_model
    config.llm_temperature = llm_temp
    config.llm_max_tokens = int(llm_tokens)
    config.mineru_enabled = mineru_enabled

    # æ›´æ–°ç¯å¢ƒå˜é‡ï¼ˆç”¨äº LangSmith ç­‰ï¼‰
    if langsmith_key:
        os.environ["LANGSMITH_API_KEY"] = langsmith_key

    logger.info(
        f"âš™ï¸ Applied config: {llm_provider}/{llm_model}, "
        f"temp={llm_temp}, MinerU={mineru_enabled}"
    )


# ============ æ ¸å¿ƒå¤„ç†å‡½æ•° ============

async def process_document_stream(
    file_obj,
    project_name: str,
    llm_provider: str,
    llm_model: str,
    llm_temp: float,
    llm_tokens: int,
    mineru_enabled: bool,
    langsmith_key: str,
    progress=gr.Progress()
):
    """
    æµå¼å¤„ç†æ–‡æ¡£ - çœŸæ­£çš„å®æ—¶æ‰§è¡Œç›‘æ§
    
    æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæƒ…å†µï¼ŒåŒ…æ‹¬ï¼š
    - æ–‡æ¡£è½¬æ¢è¯¦æƒ…ï¼ˆMinerUï¼‰
    - çŸ¥è¯†æ£€ç´¢ç»“æœ
    - å…ƒæ•°æ®ç”Ÿæˆè¿›åº¦
    - Agent æ‰§è¡ŒçŠ¶æ€
    - Critic è¯„ä¼°åé¦ˆ
    - ç½®ä¿¡åº¦å˜åŒ–
    
    æ”¯æŒåŠ¨æ€é…ç½®å‚æ•°
    """
    if file_obj is None:
        yield "âš ï¸ Please upload a file first"
        return
    
    try:
        # åº”ç”¨ç”¨æˆ·é…ç½®
        apply_user_config(
            llm_provider, llm_model, llm_temp,
            llm_tokens, mineru_enabled, langsmith_key
        )

        # åˆå§‹åŒ–çŠ¶æ€
        progress(0, desc="ğŸš€ Starting workflow...")
        output_text = "ğŸš€ **Starting FAIRifier Workflow**\n\n"
        output_text += f"âš™ï¸ **Configuration**:\n"
        output_text += f"- LLM: {llm_provider}/{llm_model}\n"
        output_text += f"- Temperature: {llm_temp}\n"
        output_text += f"- Max Tokens: {int(llm_tokens)}\n"
        output_text += f"- MinerU: {'Enabled' if mineru_enabled else 'Disabled'}\n\n"
        yield output_text
        await asyncio.sleep(0.3)

        # å‡†å¤‡æ–‡ä»¶
        file_path = file_obj.name if hasattr(file_obj, 'name') else file_obj
        project_id = f"fairifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        temp_env = create_temp_env(
            llm_provider, llm_model, llm_temp,
            llm_tokens, mineru_enabled, langsmith_key,
            project_id
        )

        progress(0.1, desc="ğŸ“„ Initializing...")
        output_text += f"ğŸ“‹ **Project ID:** `{project_id}`\n"
        output_text += f"ğŸ“ **File:** {Path(file_path).name}\n"
        output_text += f"ğŸ“ **Config File:** `{temp_env}`\n\n"
        yield output_text
        await asyncio.sleep(0.3)
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        output_path = config.output_path / project_id
        output_path.mkdir(parents=True, exist_ok=True)
        
        output_text += "---\n\n## ğŸ“Š Real-time Execution Monitor\n\n"
        yield output_text

        # è®¾ç½®æ—¥å¿—æ•è·
        log_handler = GradioLogHandler()
        log_handler.setFormatter(logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s', datefmt='%H:%M:%S'))
        
        # æ·»åŠ åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger = logging.getLogger()
        # ç§»é™¤ç°æœ‰çš„ GradioLogHandler ä»¥é¿å…é‡å¤
        for handler in root_logger.handlers[:]:
            if isinstance(handler, GradioLogHandler):
                root_logger.removeHandler(handler)
        root_logger.addHandler(log_handler)
        root_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ä¹‹å‰çš„æ—¥å¿—
        log_handler.clear()

        # çœŸæ­£è¿è¡Œ workflow å¹¶ç›‘æ§æ¯ä¸ªæ­¥éª¤
        try:
            workflow = get_workflow()
            start_time = datetime.now()
            step_count = 0
            last_log_count = 0

            # ä½¿ç”¨ LangGraph çš„ stream åŠŸèƒ½è·å–ä¸­é—´çŠ¶æ€
            # å¿…é¡»æä¾› thread_id ä»¥æ”¯æŒ checkpointer
            stream_config = {
                "configurable": {"thread_id": project_id},
                "recursion_limit": 50
            }

            async for event in workflow.workflow.astream(
                {
                    "document_path": file_path,
                    "document_content": "",
                    "document_conversion": {},
                    "output_dir": str(output_path),
                    "document_info": {},
                    "retrieved_knowledge": [],
                    "metadata_fields": [],
                    "validation_results": {},
                    "confidence_scores": {},
                    "execution_history": [],
                    "needs_human_review": False,
                    "status": "initializing"
                },
                config=stream_config
            ):
                step_count += 1
                elapsed = (datetime.now() - start_time).total_seconds()

                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ—¥å¿—ï¼ˆåœ¨è§£æäº‹ä»¶ä¹‹å‰ï¼‰
                current_logs = log_handler.get_logs()
                if current_logs:
                    log_lines = current_logs.split('\n')
                    if len(log_lines) > last_log_count:
                        new_logs = '\n'.join(log_lines[last_log_count:])
                        if new_logs.strip():
                            output_text += "### ğŸ“‹ Recent Logs\n"
                            output_text += "```\n" + new_logs + "\n```\n\n"
                            last_log_count = len(log_lines)
                            yield output_text  # å®æ—¶æ›´æ–°æ—¥å¿—

                # è§£æäº‹ä»¶
                if isinstance(event, tuple) and len(event) >= 2:
                    node_name, node_output = event[0], event[1]

                    # æ›´æ–°è¿›åº¦æ¡
                    progress_val = min(step_count / 12, 0.95)
                    progress(progress_val, desc=f"{node_name}")

                    # æ ¼å¼åŒ–èŠ‚ç‚¹ä¿¡æ¯
                    output_text += f"### ğŸ”· {node_name}\n"
                    output_text += f"â±ï¸ *Time: {elapsed:.1f}s | Step: {step_count}*\n\n"

                    if isinstance(node_output, dict):
                        # çŠ¶æ€ä¿¡æ¯
                        status = node_output.get("status", "")
                        if status:
                            status_icon = {
                                "initializing": "ğŸ”µ",
                                "parsing": "ğŸ“„",
                                "retrieving": "ğŸ”",
                                "generating": "ğŸ§ ",
                                "validating": "âœ…",
                                "complete": "âœ…",
                                "error": "âŒ"
                            }.get(status, "â–¶ï¸")
                            output_text += f"{status_icon} **Status**: `{status}`\n\n"

                        # æ–‡æ¡£è½¬æ¢ä¿¡æ¯
                        conv = node_output.get("document_conversion", {})
                        if conv.get("success"):
                            output_text += "ğŸ“„ **Document Conversion**\n"
                            output_text += f"  - âœ… Success | Method: {conv.get('method', 'N/A')}\n"
                            if "total_pages" in conv:
                                output_text += f"  - Pages: {conv['total_pages']}\n"
                            if "content_length" in conv:
                                length = conv["content_length"]
                                output_text += f"  - Content: {length:,} chars\n"
                            output_text += "\n"
                        elif conv.get("error"):
                            output_text += f"âŒ **Conversion Error**: {conv['error']}\n\n"

                        # çŸ¥è¯†æ£€ç´¢
                        knowledge = node_output.get("retrieved_knowledge", [])
                        if knowledge:
                            output_text += f"ğŸ“š **Knowledge Retrieved**: {len(knowledge)} items\n"
                            for i, item in enumerate(knowledge[:3], 1):
                                if isinstance(item, dict):
                                    output_text += f"  {i}. {item.get('title', 'N/A')}\n"
                            if len(knowledge) > 3:
                                output_text += f"  ... and {len(knowledge)-3} more\n"
                            output_text += "\n"

                        # å…ƒæ•°æ®ç”Ÿæˆ
                        fields = node_output.get("metadata_fields", [])
                        if fields:
                            output_text += f"ğŸ·ï¸ **Metadata Generated**: {len(fields)} fields\n"
                            # æŒ‰ç±»åˆ«ç»Ÿè®¡
                            categories = {}
                            for field in fields:
                                if isinstance(field, dict):
                                    cat = field.get("isa_sheet", "unassigned")
                                    categories[cat] = categories.get(cat, 0) + 1
                            for cat, count in list(categories.items())[:5]:
                                output_text += f"  - {cat}: {count} fields\n"
                            output_text += "\n"

                        # æ‰§è¡Œå†å²ï¼ˆæœ€æ–°ï¼‰
                        history = node_output.get("execution_history", [])
                        if history:
                            latest = history[-1]
                            agent = latest.get("agent_name", "Unknown")
                            success = latest.get("success", False)
                            retry = latest.get("retry_count", 0)

                            icon = "âœ…" if success else "âŒ"
                            output_text += f"{icon} **Agent**: {agent}"
                            if retry > 0:
                                output_text += f" (Retry #{retry})"
                            output_text += "\n"

                            # Critic è¯„ä¼°
                            critic = latest.get("critic_evaluation", {})
                            if critic:
                                decision = critic.get("decision", "")
                                score = critic.get("score", 0)
                                output_text += f"  - ğŸ¤– **Critic**: {decision} (score: {score:.2f})\n"

                                strengths = critic.get("strengths", [])
                                if strengths:
                                    output_text += f"  - âœ… Strengths: {', '.join(strengths[:2])}\n"

                                weaknesses = critic.get("weaknesses", [])
                                if weaknesses:
                                    output_text += f"  - âš ï¸ Weaknesses: {', '.join(weaknesses[:2])}\n"

                                suggestions = critic.get("suggestions", [])
                                if suggestions:
                                    output_text += f"  - ğŸ’¡ Suggestions: {suggestions[0]}\n"

                            output_text += "\n"

                        # ç½®ä¿¡åº¦
                        conf = node_output.get("confidence_scores", {})
                        if conf.get("overall"):
                            overall = conf["overall"]
                            output_text += f"ğŸ¯ **Confidence**: {overall:.1%}"
                            if "critic" in conf:
                                output_text += f" (Critic: {conf['critic']:.1%})"
                            output_text += "\n\n"

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æ—¥å¿—
                    current_logs = log_handler.get_logs()
                    if current_logs and len(current_logs.split('\n')) > last_log_count:
                        new_logs = '\n'.join(current_logs.split('\n')[last_log_count:])
                        if new_logs.strip():
                            output_text += "### ğŸ“‹ Recent Logs\n"
                            output_text += "```\n" + new_logs + "\n```\n\n"
                            last_log_count = len(current_logs.split('\n'))
                    
                    output_text += "---\n\n"
                    yield output_text

            # å®Œæˆ - æ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—
            total_time = (datetime.now() - start_time).total_seconds()
            progress(1.0, desc="âœ… Complete!")

            output_text += "## âœ… Workflow Complete!\n\n"
            output_text += f"â±ï¸ **Total Time**: {total_time:.1f}s\n"
            output_text += f"ğŸ“Š **Total Steps**: {step_count}\n"
            output_text += f"ğŸ“ **Output**: `{output_path}`\n\n"
            
            # æ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—
            all_logs = log_handler.get_logs()
            if all_logs:
                output_text += "### ğŸ“‹ Complete Processing Logs\n"
                output_text += "```\n" + all_logs + "\n```\n\n"

            yield output_text
            
            # æ¸…ç†æ—¥å¿—å¤„ç†å™¨
            root_logger.removeHandler(log_handler)

        except Exception as e:
            logger.error(f"Workflow execution failed: {e}", exc_info=True)
            output_text += f"\n## âŒ Execution Error\n\n"
            output_text += f"```python\n{type(e).__name__}: {str(e)}\n```\n\n"
            
            # æ˜¾ç¤ºé”™è¯¯æ—¥å¿—
            all_logs = log_handler.get_logs()
            if all_logs:
                output_text += "### ğŸ“‹ Error Logs\n"
                output_text += "```\n" + all_logs + "\n```\n\n"
            
            yield output_text
            
            # æ¸…ç†æ—¥å¿—å¤„ç†å™¨
            root_logger.removeHandler(log_handler)
            
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        error_msg = f"âŒ **Error:** {str(e)}\n\n"
        
        # å°è¯•è·å–æ—¥å¿—
        try:
            root_logger = logging.getLogger()
            for handler in root_logger.handlers:
                if isinstance(handler, GradioLogHandler):
                    logs = handler.get_logs()
                    if logs:
                        error_msg += "### ğŸ“‹ Error Logs\n```\n" + logs + "\n```\n"
                    root_logger.removeHandler(handler)
                    break
        except:
            pass
        
        yield error_msg


async def process_document_full(
    file_obj,
    project_name: str,
    llm_provider: str,
    llm_model: str,
    llm_temp: float,
    llm_tokens: int,
    mineru_enabled: bool,
    langsmith_key: str
) -> Tuple[Optional[Dict], str, str]:
    """
    å®Œæ•´å¤„ç†ï¼ˆè¿”å›ç»“æ„åŒ–ç»“æœç”¨äºå…¶ä»–æ ‡ç­¾é¡µï¼‰
    æ”¯æŒåŠ¨æ€é…ç½®å‚æ•°
    """
    if file_obj is None:
        return None, "âš ï¸ Please upload a file first", ""
    
    try:
        # åº”ç”¨ç”¨æˆ·é…ç½®
        apply_user_config(
            llm_provider, llm_model, llm_temp,
            llm_tokens, mineru_enabled, langsmith_key
        )

        file_path = file_obj.name if hasattr(file_obj, 'name') else file_obj
        project_id = f"fairifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
        temp_env = create_temp_env(
            llm_provider, llm_model, llm_temp,
            llm_tokens, mineru_enabled, langsmith_key,
            project_id
        )

        # è¾“å‡ºç›®å½•
        output_path = config.output_path / project_id
        output_path.mkdir(parents=True, exist_ok=True)

        # è¿è¡Œ workflowï¼ˆä½¿ç”¨æ›´æ–°åçš„ configï¼‰
        workflow = get_workflow()
        result = await workflow.run(file_path, project_id, output_dir=str(output_path))
        
        status = result.get("status", "unknown")
        
        status_msg = f"âœ… Processing completed! Status: {status.upper()}"
        if result.get("needs_human_review"):
            status_msg += " (âš ï¸ Needs review)"
        
        # è¿”å›: (å®Œæ•´ç»“æœJSON, çŠ¶æ€æ¶ˆæ¯, è¾“å‡ºç›®å½•)
        return result, status_msg, str(output_path)
        
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        return None, f"âŒ Error: {str(e)}", ""


# ============ ç»“æœå±•ç¤ºå‡½æ•° ============

def format_execution_history(result: Dict) -> str:
    """æ ¼å¼åŒ–æ‰§è¡Œå†å²ä¸º Markdown è¡¨æ ¼"""
    if not result or "execution_history" not in result:
        return "No execution history available"
    
    history = result["execution_history"]
    
    # åˆ›å»º Markdown è¡¨æ ¼
    table = "| Step | Agent | Success | Critic Decision | Score |\n"
    table += "|------|-------|---------|-----------------|-------|\n"
    
    for i, exec_record in enumerate(history, 1):
        agent_name = exec_record.get("agent_name", "Unknown")
        success = "âœ…" if exec_record.get("success", False) else "âŒ"
        
        critic_eval = exec_record.get("critic_evaluation", {})
        decision = critic_eval.get("decision", "N/A")
        score = critic_eval.get("score", 0.0)
        score_str = f"{score:.2f}" if score else "N/A"
        
        table += f"| {i} | {agent_name} | {success} | {decision} | {score_str} |\n"
    
    return table


def format_confidence_details(result: Dict) -> str:
    """æ ¼å¼åŒ–ç½®ä¿¡åº¦è¯¦æƒ…"""
    if not result or "confidence_scores" not in result:
        return "No confidence scores available"
    
    conf_scores = result["confidence_scores"]
    
    md = "## ğŸ¯ Confidence Breakdown\n\n"
    
    # æ€»ä½“ç½®ä¿¡åº¦
    overall = conf_scores.get("overall", 0.0)
    md += f"### Overall Confidence: **{overall:.2%}**\n\n"
    
    # å„ç»´åº¦
    md += "### Dimensions\n\n"
    md += f"- **Critic Confidence:** {conf_scores.get('critic', 0.0):.2%}\n"
    md += f"- **Structural Confidence:** {conf_scores.get('structural', 0.0):.2%}\n"
    md += f"- **Validation Confidence:** {conf_scores.get('validation', 0.0):.2%}\n\n"
    
    # è´¨é‡æŒ‡æ ‡
    if "quality_metrics" in result:
        md += "### Quality Metrics\n\n"
        md += "```json\n"
        md += json.dumps(result["quality_metrics"], indent=2)
        md += "\n```\n"
    
    return md


def format_metadata_fields(result: Dict) -> Tuple[str, str]:
    """
    æ ¼å¼åŒ–å…ƒæ•°æ®å­—æ®µ
    è¿”å›: (Markdownå±•ç¤º, JSONå­—ç¬¦ä¸²)
    """
    if not result or "metadata_fields" not in result:
        return "No metadata fields available", "{}"
    
    fields = result["metadata_fields"]
    if isinstance(fields, dict):
        fields = list(fields.values())
    
    # ç»Ÿè®¡
    total = len(fields)
    required = sum(1 for f in fields if isinstance(f, dict) and f.get("required", False))
    
    # æŒ‰ ISA sheet åˆ†ç»„
    from collections import defaultdict
    by_sheet = defaultdict(list)
    
    for field in fields:
        if isinstance(field, dict):
            sheet = field.get("isa_sheet", "unassigned")
            by_sheet[sheet].append(field)
    
    # Markdown
    md = f"## ğŸ·ï¸ Metadata Fields ({total} total, {required} required)\n\n"
    
    for sheet_name in ["investigation", "study", "assay", "sample", "observationunit", "unassigned"]:
        if sheet_name not in by_sheet:
            continue
        
        sheet_fields = by_sheet[sheet_name]
        md += f"### {sheet_name.upper()} ({len(sheet_fields)} fields)\n\n"
        
        for field in sheet_fields[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            name = field.get("field_name", "Unknown")
            required_badge = "ğŸ”´" if field.get("required") else "âšª"
            conf = field.get("confidence", 0.0)
            
            md += f"- {required_badge} **{name}** (confidence: {conf:.2%})\n"
        
        if len(sheet_fields) > 10:
            md += f"\n_... and {len(sheet_fields) - 10} more fields_\n"
        
        md += "\n"
    
    # JSON
    json_str = json.dumps({"metadata_fields": fields}, indent=2)
    
    return md, json_str


# ============ Gradio ç•Œé¢ ============

def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks() as demo:
        
        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ§¬ FAIRifier - FAIR Metadata Generator
        
        Automated generation of FAIR metadata from research documents using LangGraph + LLM
        
        ---
        """)
        
        # å…¨å±€çŠ¶æ€ï¼ˆå­˜å‚¨å¤„ç†ç»“æœï¼‰
        result_state = gr.State(None)
        output_dir_state = gr.State("")
        
        # ä¸»æ ‡ç­¾é¡µ
        with gr.Tabs():
            
            # ========== Tab 1: Upload & Process ==========
            with gr.Tab("ğŸ“„ Upload & Process", id="upload"):
                gr.Markdown("### Upload your research document and start processing")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        file_input = gr.File(
                            label="ğŸ“ Upload Document",
                            file_types=[".pdf", ".txt", ".md"],
                            type="filepath"
                        )
                        
                        project_name = gr.Textbox(
                            label="ğŸ·ï¸ Project Name (optional)",
                            placeholder="e.g., Earthworm Genome Study",
                            value=""
                        )
                        
                        with gr.Row():
                            process_btn = gr.Button(
                                "ğŸš€ Process Document",
                                variant="primary",
                                size="lg"
                            )
                            gr.ClearButton(
                                components=[file_input, project_name]
                            )
                        
                        gr.Markdown("""
                        **Supported formats:**
                        - PDF (with MinerU high-fidelity parsing)
                        - Text files (.txt)
                        - Markdown (.md)
                        """)
                    
                    with gr.Column(scale=2):
                        # æµå¼è¾“å‡ºæ˜¾ç¤º
                        stream_output = gr.Markdown(
                            value="**Ready to process.** Upload a file and click the button above.",
                            label="Processing Progress",
                            elem_classes=["output-markdown"]
                        )
                
                # ç¤ºä¾‹æ–‡ä»¶
                example_base = Path(__file__).parent.parent.parent.parent / "examples" / "inputs"
                example_files = []
                
                # å®šä¹‰ç¤ºä¾‹æ–‡ä»¶åŠå…¶å¯èƒ½çš„è·¯å¾„
                example_configs = [
                    {
                        "name": "earthworm_4n_paper_bioRxiv.pdf",
                        "display": "Earthworm 4n Paper",
                        "paths": [
                            example_base / "earthworm_4n_paper_bioRxiv.pdf",
                            example_base.parent.parent / "evaluation" / "datasets" / "raw" / "earthworm" / "earthworm_4n_paper_bioRxiv.pdf"
                        ]
                    },
                    {
                        "name": "aec8570_CombinedPDF_v1.pdf",
                        "display": "Biosensor Paper",
                        "paths": [
                            example_base / "aec8570_CombinedPDF_v1.pdf",
                            example_base.parent.parent / "evaluation" / "datasets" / "raw" / "biosensor" / "aec8570_CombinedPDF_v1.pdf"
                        ]
                    }
                ]
                
                for example_config in example_configs:
                    for path in example_config["paths"]:
                        if path.exists():
                            example_files.append([str(path), example_config["display"]])
                            break
                
                if example_files:
                    gr.Examples(
                        examples=example_files,
                        inputs=[file_input, project_name],
                        label="ğŸ“š Example Documents"
                    )
            
            # ========== Tab 2: Results Summary ==========
            with gr.Tab("ğŸ“Š Results Summary", id="results"):
                gr.Markdown("### Processing Results Overview")
                
                status_display = gr.Textbox(
                    label="Status",
                    value="No results yet",
                    interactive=False
                )
                
                with gr.Row():
                    conf_critic = gr.Number(
                        label="ğŸ¤– Critic Confidence",
                        value=0,
                        interactive=False
                    )
                    conf_structural = gr.Number(
                        label="ğŸ—ï¸ Structural Confidence",
                        value=0,
                        interactive=False
                    )
                    conf_validation = gr.Number(
                        label="âœ… Validation Confidence",
                        value=0,
                        interactive=False
                    )
                    conf_overall = gr.Number(
                        label="ğŸ¯ Overall Confidence",
                        value=0,
                        interactive=False
                    )
                
                # æ‰§è¡Œæ‘˜è¦
                with gr.Row():
                    exec_total = gr.Number(
                        label="Total Steps",
                        value=0,
                        interactive=False
                    )
                    exec_success = gr.Number(
                        label="Successful Steps",
                        value=0,
                        interactive=False
                    )
                    exec_failed = gr.Number(
                        label="Failed Steps",
                        value=0,
                        interactive=False
                    )
                    exec_retry = gr.Number(
                        label="Steps with Retry",
                        value=0,
                        interactive=False
                    )
                
                # å®Œæ•´ç»“æœï¼ˆJSONï¼‰
                result_json = gr.JSON(
                    label="Full Results (JSON)",
                    visible=False
                )
                
                show_json_btn = gr.Button("ğŸ“‹ Show Full JSON")
                
                def toggle_json(visible):
                    return gr.update(visible=not visible)
                
                show_json_btn.click(
                    fn=lambda: gr.update(visible=True),
                    outputs=result_json,
                    queue=False  # å¿«é€Ÿ UI æ›´æ–°ä¸éœ€è¦é˜Ÿåˆ—
                )
            
            # ========== Tab 3: Execution History ==========
            with gr.Tab("ğŸ” Execution History", id="history"):
                gr.Markdown("### Step-by-Step Execution Timeline with Critic Evaluations")
                
                history_output = gr.Markdown(
                    value="No execution history yet. Process a document first.",
                    label="Execution Timeline"
                )
            
            # ========== Tab 4: Confidence Details ==========
            with gr.Tab("ğŸ¯ Confidence Details", id="confidence"):
                gr.Markdown("### Multi-Dimensional Confidence Analysis")
                
                confidence_output = gr.Markdown(
                    value="No confidence data yet. Process a document first.",
                    label="Confidence Breakdown"
                )
            
            # ========== Tab 5: Metadata Fields ==========
            with gr.Tab("ğŸ·ï¸ Metadata Fields", id="metadata"):
                gr.Markdown("### Generated FAIR-DS Metadata Fields")
                
                metadata_output = gr.Markdown(
                    value="No metadata fields yet. Process a document first.",
                    label="Metadata Fields by ISA Sheet"
                )
                
                metadata_json_output = gr.Code(
                    language="json",
                    label="Metadata JSON",
                    visible=False
                )
                
                show_metadata_json_btn = gr.Button("ğŸ“‹ Show Metadata JSON")
                show_metadata_json_btn.click(
                    fn=lambda: gr.update(visible=True),
                    outputs=metadata_json_output,
                    queue=False  # å¿«é€Ÿ UI æ›´æ–°ä¸éœ€è¦é˜Ÿåˆ—
                )
            
            # ========== Tab 6: Download Artifacts ==========
            with gr.Tab("ğŸ“¦ Download", id="download"):
                gr.Markdown("### Download Generated Artifacts")
                
                output_dir_display = gr.Textbox(
                    label="Output Directory",
                    value="",
                    interactive=False
                )
                
                with gr.Row():
                    gr.File(
                        label="ğŸ“„ Metadata JSON (FAIR-DS)",
                        interactive=False
                    )
                    gr.File(
                        label="ğŸ“Š Workflow Report",
                        interactive=False
                    )
        
        # ========== é…ç½®åŒºï¼ˆå¯äº¤äº’ï¼‰ ==========
        with gr.Accordion("âš™ï¸ Configuration", open=False):
            gr.Markdown("""
            ### LLM and Environment Settings
            ğŸ’¡ **Tip**: ä¿®æ”¹è¿™äº›å‚æ•°ä¼šåœ¨å¤„ç†æ—¶åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œ
            è¦†ç›–é»˜è®¤çš„ .env é…ç½®
            """)

            with gr.Row():
                llm_provider_input = gr.Dropdown(
                    choices=["openai", "anthropic", "qwen", "ollama"],
                    label="LLM Provider",
                    value=config.llm_provider,
                    interactive=True
                )
                llm_model_input = gr.Textbox(
                    label="Model Name",
                    value=config.llm_model,
                    interactive=True
                )

            with gr.Row():
                llm_temp_input = gr.Slider(
                    minimum=0.0,
                    maximum=2.0,
                    step=0.1,
                    label="Temperature",
                    value=config.llm_temperature,
                    interactive=True
                )
                llm_tokens_input = gr.Number(
                    label="Max Tokens",
                    value=config.llm_max_tokens,
                    interactive=True
                )

            mineru_enabled_input = gr.Checkbox(
                label="Enable MinerU (High-fidelity PDF parsing)",
                value=config.mineru_enabled,
                interactive=True
            )

            langsmith_key_input = gr.Textbox(
                label="LangSmith API Key (for tracing)",
                type="password",
                value=os.getenv("LANGSMITH_API_KEY", ""),
                interactive=True,
                placeholder="sk-..."
            )
        
        # ========== äº‹ä»¶ç»‘å®š ==========

        # æµå¼å¤„ç† - åªæ›´æ–°è¿›åº¦æ˜¾ç¤ºï¼ˆä¼ å…¥é…ç½®å‚æ•°ï¼‰
        # ä½¿ç”¨ show_progress æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œä½¿ç”¨ concurrency_limit é™åˆ¶å¹¶å‘
        process_btn.click(
            fn=process_document_stream,
            inputs=[
                file_input, project_name,
                llm_provider_input, llm_model_input,
                llm_temp_input, llm_tokens_input,
                mineru_enabled_input, langsmith_key_input
            ],
            outputs=[stream_output],
            api_name="process_stream",
            show_progress="full",  # æ˜¾ç¤ºå®Œæ•´è¿›åº¦æ¡
            concurrency_limit=1,  # é™åˆ¶åŒæ—¶åªèƒ½å¤„ç†ä¸€ä¸ªæ–‡æ¡£
            concurrency_id="document_processing"  # å…±äº«é˜Ÿåˆ—
        ).then(
            # å¤„ç†å®Œæˆåï¼Œè·å–å®Œæ•´ç»“æœå¹¶æ›´æ–°æ‰€æœ‰æ ‡ç­¾é¡µ
            fn=process_document_full,
            inputs=[
                file_input, project_name,
                llm_provider_input, llm_model_input,
                llm_temp_input, llm_tokens_input,
                mineru_enabled_input, langsmith_key_input
            ],
            outputs=[result_state, status_display, output_dir_state],
            concurrency_id="document_processing"  # å…±äº«é˜Ÿåˆ—
        ).then(
            # æ›´æ–°ç»“æœæ‘˜è¦ï¼ˆå¿«é€Ÿæ“ä½œï¼Œä¸éœ€è¦é˜Ÿåˆ—ï¼‰
            fn=lambda r: (
                r if r else None,
                r.get("confidence_scores", {}).get("critic", 0.0) if r else 0.0,
                r.get("confidence_scores", {}).get("structural", 0.0) if r else 0.0,
                r.get("confidence_scores", {}).get("validation", 0.0) if r else 0.0,
                r.get("confidence_scores", {}).get("overall", 0.0) if r else 0.0,
                r.get("execution_summary", {}).get("total_steps", 0) if r else 0,
                r.get("execution_summary", {}).get("successful_steps", 0) if r else 0,
                r.get("execution_summary", {}).get("failed_steps", 0) if r else 0,
                r.get("execution_summary", {}).get("steps_requiring_retry", 0) if r else 0,
            ),
            inputs=[result_state],
            outputs=[
                result_json,
                conf_critic,
                conf_structural,
                conf_validation,
                conf_overall,
                exec_total,
                exec_success,
                exec_failed,
                exec_retry,
            ],
            queue=False  # å¿«é€Ÿ UI æ›´æ–°ä¸éœ€è¦é˜Ÿåˆ—
        ).then(
            # æ›´æ–°æ‰§è¡Œå†å²ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            fn=format_execution_history,
            inputs=[result_state],
            outputs=[history_output],
            queue=False
        ).then(
            # æ›´æ–°ç½®ä¿¡åº¦è¯¦æƒ…ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            fn=format_confidence_details,
            inputs=[result_state],
            outputs=[confidence_output],
            queue=False
        ).then(
            # æ›´æ–°å…ƒæ•°æ®å­—æ®µï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            fn=format_metadata_fields,
            inputs=[result_state],
            outputs=[metadata_output, metadata_json_output],
            queue=False
        ).then(
            # æ›´æ–°è¾“å‡ºç›®å½•ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            fn=lambda d: d,
            inputs=[output_dir_state],
            outputs=[output_dir_display],
            queue=False
        )
    
    return demo


# ============ ä¸»å‡½æ•° ============

def main():
    """å¯åŠ¨ Gradio åº”ç”¨"""
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åº”ç”¨ç¯å¢ƒå˜é‡é…ç½®
    apply_env_overrides(config)
    
    # åˆ›å»ºç•Œé¢
    demo = create_gradio_interface()
    
    # é…ç½®é˜Ÿåˆ—ï¼ˆæ ¹æ® Gradio æœ€ä½³å®è·µï¼‰
    # max_size: æœ€å¤§é˜Ÿåˆ—å¤§å°ï¼Œé˜²æ­¢ç”¨æˆ·ç­‰å¾…æ—¶é—´è¿‡é•¿
    # default_concurrency_limit: é»˜è®¤å¹¶å‘é™åˆ¶ï¼Œæ§åˆ¶èµ„æºä½¿ç”¨
    demo.queue(
        max_size=5,  # æœ€å¤šæ’é˜Ÿ 5 ä¸ªè¯·æ±‚
        default_concurrency_limit=1  # åŒæ—¶åªå¤„ç† 1 ä¸ªæ–‡æ¡£ï¼ˆå› ä¸ºå¤„ç†å¾ˆè€—æ—¶ï¼‰
    )
    
    # å¯åŠ¨æœåŠ¡
    logger.info("ğŸš€ Starting Gradio UI...")
    logger.info(f"ğŸ“Š LLM: {config.llm_provider}/{config.llm_model}")
    logger.info(f"ğŸ”§ MinerU: {'Enabled' if config.mineru_enabled else 'Disabled'}")
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,  # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        max_threads=40,  # å¢åŠ çº¿ç¨‹æ± å¤§å°ä»¥æ”¯æŒæ›´å¤šå¹¶å‘
        footer_links=["api", "gradio", "settings"],  # Gradio 6: æ›¿ä»£ show_api
    )


if __name__ == "__main__":
    main()


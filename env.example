# =============================================================================
# FAIRiAgent Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your actual values.
# All options below are optional unless marked (Required).
# Do not commit .env to version control.

# =============================================================================
# LangSmith (Optional)
# =============================================================================
# Set LANGSMITH_API_KEY to enable tracing. Disable: LANGSMITH_DISABLE=1 or LANGCHAIN_TRACING_V2=false
# LANGSMITH_API_KEY=
# LANGSMITH_PROJECT=fairifier
# LANGSMITH_USE_FAIR_NAMING=true
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# =============================================================================
# LLM Provider (Required for API providers)
# =============================================================================
# Supported: ollama, openai, qwen, anthropic
LLM_PROVIDER=ollama
# Model name (e.g. qwen3:30b, gpt-4o, qwen3-max-2026-01-23, claude-sonnet-4)
FAIRIFIER_LLM_MODEL=qwen3:30b
# Base URL: Ollama default http://localhost:11434; overridden per-provider below if set
FAIRIFIER_LLM_BASE_URL=http://localhost:11434

# API Key (Required for openai, qwen, anthropic; leave empty for ollama)
# OpenAI: https://platform.openai.com/api-keys
# Qwen/DashScope: https://dashscope.console.aliyun.com/
# Anthropic: https://console.anthropic.com/
LLM_API_KEY=your_api_key_here

# Provider-specific base URL (optional; defaults used if not set)
# OPENAI_API_BASE_URL=https://api.openai.com/v1
# QWEN_API_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1

# =============================================================================
# LLM Parameters (Optional) â€” keep consistent across configs for control variable
# =============================================================================
# Temperature: 0.3 recommended for structured extraction (JSON/metadata); lower = more deterministic
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=100000
# Thinking mode (streaming; some models only)
# LLM_ENABLE_THINKING=false

# =============================================================================
# Document Parsing Context Limits (Optional)
# =============================================================================
# Max characters sent to LLM for Markdown / raw text (defaults: 600000, 500000)
# MAX_DOC_CONTEXT_MARKDOWN=600000
# MAX_DOC_CONTEXT_TEXT=500000

# =============================================================================
# External Services (Optional)
# =============================================================================
# Vector DB (e.g. Qdrant)
# QDRANT_URL=http://localhost:6333

# FAIR Data Station API (Swagger: http://localhost:8083/swagger-ui/index.html)
FAIR_DS_API_URL=http://localhost:8083

# =============================================================================
# Processing Limits (Optional)
# =============================================================================
FAIRIFIER_MAX_DOCUMENT_SIZE_MB=50
FAIRIFIER_MAX_PROCESSING_TIME_MINUTES=10
# Minimum overall confidence to accept workflow result
FAIRIFIER_MIN_CONFIDENCE_THRESHOLD=0.75

# =============================================================================
# Retry (Optional)
# =============================================================================
FAIRIFIER_MAX_STEP_RETRIES=2
FAIRIFIER_MAX_GLOBAL_RETRIES=5

# =============================================================================
# Critic Rubric & Thresholds (Optional)
# =============================================================================
# Path to critic rubric YAML
# FAIRIFIER_CRITIC_RUBRIC_PATH=docs/en/development/critic_rubric.yaml

# ACCEPT threshold per agent (score >= threshold => accept)
FAIRIFIER_CRITIC_ACCEPT_THRESHOLD_DOCUMENT_PARSER=0.75
FAIRIFIER_CRITIC_ACCEPT_THRESHOLD_KNOWLEDGE_RETRIEVER=0.7
FAIRIFIER_CRITIC_ACCEPT_THRESHOLD_JSON_GENERATOR=0.75
FAIRIFIER_CRITIC_ACCEPT_THRESHOLD_GENERAL=0.7
# RETRY band: score in [min, max] => retry; below min => escalate
FAIRIFIER_CRITIC_RETRY_MIN_THRESHOLD=0.4
FAIRIFIER_CRITIC_RETRY_MAX_THRESHOLD=0.69

# =============================================================================
# Confidence Aggregation (Optional)
# =============================================================================
# Weights for overall confidence: critic, structural, validation
# FAIRIFIER_CONF_WEIGHT_CRITIC=0.5
# FAIRIFIER_CONF_WEIGHT_STRUCTURAL=0.3
# FAIRIFIER_CONF_WEIGHT_VALIDATION=0.2
# Coverage targets
# FAIRIFIER_STRUCTURAL_COVERAGE_TARGET=0.75
# FAIRIFIER_EVIDENCE_COVERAGE_TARGET=0.7
# FAIRIFIER_VALIDATION_PASS_TARGET=0.8

# =============================================================================
# MinerU Document Conversion (Optional)
# =============================================================================
# Requires MinerU HTTP server (e.g. https://github.com/opendatalab/MinerU)
MINERU_ENABLED=false
# MINERU_SERVER_URL=http://localhost:30000
# MINERU_CLI_PATH=mineru
# MINERU_BACKEND=vlm-http-client
# MINERU_TIMEOUT_SECONDS=300

# =============================================================================
# Checkpointer (Optional)
# =============================================================================
# Options: none (stateless), memory (dev only), sqlite (production)
CHECKPOINTER_BACKEND=sqlite
# CHECKPOINT_DB_PATH=output/.checkpoints.db

# =============================================================================
# Mem0 Memory Layer (Optional)
# =============================================================================
# Persistent semantic memory; requires Qdrant. Complements SQLite checkpointer.
# Mem0 uses its own LLM source (provider/model/base URL) independent of main LLM.
MEM0_ENABLED=false
# If true and MEM0_ENABLED=true, raise and exit when mem0 init fails
# MEM0_STRICT=false
# mem0 LLM source (mem0-supported only: ollama, openai, anthropic)
# MEM0_LLM_PROVIDER=ollama
# MEM0_LLM_MODEL=  (defaults to main LLM model if not set)
# MEM0_LLM_BASE_URL=  (for openai; e.g. OpenAI-compatible API base)
# MEM0_LLM_API_KEY=  (for openai or anthropic)
# MEM0_OLLAMA_BASE_URL=http://localhost:11434  (for ollama; also used for embedder)
# MEM0_EMBEDDING_MODEL=nomic-embed-text
# MEM0_EMBEDDING_DIMS=768  (nomic-embed-text=768; OpenAI ada-002=1536; must match embedder)
# MEM0_QDRANT_HOST=localhost
# MEM0_QDRANT_PORT=6333
# Or set URL: MEM0_QDRANT_URL=http://localhost:6333
# MEM0_COLLECTION_NAME=fairifier_memories

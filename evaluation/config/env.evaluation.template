# FAIRiAgent Evaluation Configuration Template
# Copy this file to env.evaluation and fill in your credentials

# ============================================
# LangSmith Tracking (REQUIRED)
# ============================================
LANGSMITH_API_KEY=your_langsmith_key_here
LANGSMITH_PROJECT=fairifier-evaluation
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# ============================================
# Evaluation Settings
# ============================================
EVAL_GROUND_TRUTH_PATH=evaluation/datasets/annotated/ground_truth_v1.json
EVAL_OUTPUT_DIR=evaluation/runs/run_20251121
EVAL_NUM_WORKERS=2  # Number of parallel FAIRiAgent instances

# ============================================
# LLM-as-Judge Configuration (for evaluation)
# ============================================
# This LLM is used to evaluate FAIRiAgent outputs
# Can be different from the models being tested
EVAL_JUDGE_PROVIDER=anthropic  # anthropic, openai, ollama
EVAL_JUDGE_MODEL=claude-sonnet-4
EVAL_JUDGE_API_KEY=your_anthropic_key_here
EVAL_JUDGE_TEMPERATURE=0.0
EVAL_JUDGE_MAX_TOKENS=100000

# ============================================
# FAIR Data Station API (optional)
# ============================================
FAIR_DS_API_URL=http://localhost:8083

# ============================================
# Evaluation Options
# ============================================
# Run only specific evaluators (comma-separated)
# Options: completeness,correctness,schema,ontology,llm_judge
# Leave empty to run all
EVAL_RUN_EVALUATORS=

# Enable/disable specific features
EVAL_ENABLE_ONTOLOGY_VALIDATION=true
EVAL_ENABLE_SCHEMA_VALIDATION=true
EVAL_COMPUTE_CORRELATIONS=true

# Statistical analysis
EVAL_CONFIDENCE_LEVEL=0.95  # For significance tests
EVAL_MIN_SAMPLES_FOR_STATS=10


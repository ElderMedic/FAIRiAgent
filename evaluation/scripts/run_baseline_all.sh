#!/bin/bash

######################################################################
# Run baseline single-prompt evaluations on all documents
# This serves as a comparison against the multi-agent workflow
######################################################################

set -e

# æ¿€æ´»ç¯å¢ƒ
source /Users/changlinke/miniforge3/etc/profile.d/conda.sh
conda activate FAIRiAgent

# é…ç½®
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BASE_OUTPUT_DIR="evaluation/runs/baseline_${TIMESTAMP}"
GROUND_TRUTH="evaluation/datasets/annotated/ground_truth_filtered.json"  # Excludes biorem
N_RUNS=10  # æ¯ä¸ªæ–‡æ¡£è¿è¡Œ 10 æ¬¡
WORKERS=3  # å¹¶å‘æ•°ï¼ˆæ¯” agentic ä½ï¼Œå› ä¸ºå•æ¬¡è°ƒç”¨æ›´å¿«ï¼‰

echo "========================================================================"
echo "ğŸ”¬ Baseline Single-Prompt Evaluation"
echo "========================================================================"
echo "Output directory: $BASE_OUTPUT_DIR"
echo "Ground truth: $GROUND_TRUTH"
echo "Runs per document: $N_RUNS"
echo "Workers: $WORKERS"
echo "========================================================================"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$BASE_OUTPUT_DIR"

# å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹é…ç½®
# ä½¿ç”¨ GPT-4o ä½œä¸º baselineï¼šå¹¿æ³›ä½¿ç”¨ï¼Œæ—  agent/thinking åŠŸèƒ½
declare -a CONFIGS=(
    "openai_gpt4o:evaluation/config/model_configs/openai_gpt4o.env"
)

# å¦‚æœéœ€è¦é¢å¤–å¯¹æ¯”ï¼Œå¯ä»¥æ·»åŠ ï¼š
# "anthropic_sonnet:evaluation/config/model_configs/anthropic_sonnet.env"

# è¿è¡Œæ¯ä¸ªé…ç½®
for config_pair in "${CONFIGS[@]}"; do
    IFS=':' read -r config_name config_file <<< "$config_pair"
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "ğŸ“‹ Running baseline: $config_name"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    python evaluation/scripts/run_baseline_batch.py \
        --config-file "$config_file" \
        --config-name "baseline_${config_name}" \
        --ground-truth "$GROUND_TRUTH" \
        --output-dir "$BASE_OUTPUT_DIR/baseline_${config_name}" \
        --workers "$WORKERS" \
        --n-runs "$N_RUNS"
    
    if [ $? -eq 0 ]; then
        echo "âœ… baseline_${config_name} completed"
    else
        echo "âŒ baseline_${config_name} failed"
    fi
done

echo ""
echo "========================================================================"
echo "âœ… All baseline evaluations complete!"
echo "========================================================================"
echo "Output directory: $BASE_OUTPUT_DIR"
echo ""
echo "Next steps:"
echo "1. Run evaluators on baseline outputs:"
echo "   python evaluation/scripts/evaluate_outputs.py \\"
echo "     --run-dir $BASE_OUTPUT_DIR/baseline_openai_gpt4o \\"
echo "     --ground-truth $GROUND_TRUTH \\"
echo "     --output-dir $BASE_OUTPUT_DIR/baseline_openai_gpt4o/evaluation_results"
echo ""
echo "2. Compare with agentic workflow results using analysis tools"
echo "========================================================================"


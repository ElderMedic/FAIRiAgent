#!/usr/bin/env python3
"""è¯Šæ–­è¯„ä¼°æ•°æ®é›†å’Œåˆ†æç»“æœçš„é—®é¢˜"""

import json
import sys
from pathlib import Path
from collections import defaultdict

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from evaluation.analysis.config import (
    EXCLUDED_MODELS,
    EXCLUDED_DOCUMENTS,
    EXCLUDED_DIRECTORIES,
)


def check_ground_truth():
    """æ£€æŸ¥ ground truth æ•°æ®é›†çš„é—®é¢˜"""
    print("=" * 70)
    print("1. Ground Truth æ•°æ®é›†æ£€æŸ¥")
    print("=" * 70)
    
    # Try multiple possible paths
    possible_paths = [
        Path("evaluation/datasets/annotated/ground_truth_filtered.json"),
        Path(__file__).parent.parent / "datasets/annotated/ground_truth_filtered.json",
        Path(__file__).parent.parent.parent / "evaluation/datasets/annotated/ground_truth_filtered.json",
    ]
    
    gt_file = None
    for path in possible_paths:
        if path.exists():
            gt_file = path
            break
    
    if not gt_file or not gt_file.exists():
        print(f"âŒ Ground truth æ–‡ä»¶ä¸å­˜åœ¨")
        for path in possible_paths:
            print(f"   å°è¯•: {path} ({path.exists()})")
        return []
    
    with open(gt_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    documents = data.get("documents", [])
    print(f"\nâœ… æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    issues = []
    for doc in documents:
        doc_id = doc.get("document_id", "unknown")
        print(f"\nğŸ“„ æ–‡æ¡£: {doc_id}")
        
        # æ£€æŸ¥æ–‡æ¡£è·¯å¾„
        doc_path = doc.get("document_path", "")
        if doc_path:
            full_path = Path(doc_path)
            if not full_path.is_absolute():
                full_path = Path("evaluation") / doc_path
            
            if full_path.exists():
                print(f"   âœ… æ–‡æ¡£è·¯å¾„å­˜åœ¨: {full_path}")
            else:
                print(f"   âŒ æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {full_path}")
                issues.append(f"{doc_id}: æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨")
                
                # å°è¯•æŸ¥æ‰¾æ›¿ä»£è·¯å¾„
                doc_name = Path(doc_path).stem
                possible_paths = [
                    Path(f"evaluation/datasets/raw/{doc_id}/{doc_name}.pdf"),
                    Path(f"evaluation/datasets/raw/{doc_id}/{doc_name}.md"),
                ]
                for alt_path in possible_paths:
                    if alt_path.exists():
                        print(f"   ğŸ’¡ å»ºè®®ä½¿ç”¨: {alt_path}")
                        break
        else:
            print(f"   âš ï¸  æ–‡æ¡£è·¯å¾„æœªè®¾ç½®")
            issues.append(f"{doc_id}: æ–‡æ¡£è·¯å¾„æœªè®¾ç½®")
        
        # æ£€æŸ¥å­—æ®µ
        fields = doc.get("ground_truth_fields", [])
        print(f"   ğŸ“Š å­—æ®µæ•°: {len(fields)}")
        
        required = sum(1 for f in fields if f.get("is_required", False))
        recommended = sum(1 for f in fields if f.get("is_recommended", False))
        print(f"   - å¿…éœ€å­—æ®µ: {required}")
        print(f"   - æ¨èå­—æ®µ: {recommended}")
        
        # æ£€æŸ¥å­—æ®µç»“æ„
        missing_fields = []
        for i, field in enumerate(fields[:10]):  # æ£€æŸ¥å‰10ä¸ª
            if "field_name" not in field:
                missing_fields.append(f"å­—æ®µ {i}: ç¼ºå°‘ field_name")
            if "isa_sheet" not in field:
                missing_fields.append(f"å­—æ®µ {i}: ç¼ºå°‘ isa_sheet")
        
        if missing_fields:
            print(f"   âš ï¸  å­—æ®µç»“æ„é—®é¢˜: {len(missing_fields)} ä¸ª")
            issues.extend([f"{doc_id}: {m}" for m in missing_fields[:3]])
    
    return issues


def check_runs_data():
    """æ£€æŸ¥è¿è¡Œæ•°æ®çš„å®Œæ•´æ€§"""
    print("\n" + "=" * 70)
    print("2. è¿è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
    print("=" * 70)
    
    # Try multiple possible paths
    possible_paths = [
        Path("evaluation/runs"),
        Path(__file__).parent.parent / "runs",
        Path(__file__).parent.parent.parent / "evaluation/runs",
    ]
    
    runs_dir = None
    for path in possible_paths:
        if path.exists():
            runs_dir = path
            break
    
    if not runs_dir or not runs_dir.exists():
        print(f"âŒ Runs ç›®å½•ä¸å­˜åœ¨")
        for path in possible_paths:
            print(f"   å°è¯•: {path} ({path.exists()})")
        return {}
    
    model_stats = defaultdict(lambda: {"complete": 0, "incomplete": 0, "missing_eval": 0})
    doc_stats = defaultdict(lambda: {"complete": 0, "incomplete": 0})
    
    for model_dir in runs_dir.iterdir():
        if not model_dir.is_dir() or model_dir.name in EXCLUDED_DIRECTORIES:
            continue
        
        model = model_dir.name
        if model in EXCLUDED_MODELS:
            continue
        
        for doc_dir in model_dir.iterdir():
            if not doc_dir.is_dir():
                continue
            
            doc_id = doc_dir.name
            if doc_id in EXCLUDED_DOCUMENTS:
                continue
            
            for run_dir in doc_dir.glob("run_*"):
                if not run_dir.is_dir():
                    continue
                
                has_metadata = (run_dir / "metadata_json.json").exists()
                has_eval = (run_dir / "eval_result.json").exists()
                
                if has_metadata and has_eval:
                    model_stats[model]["complete"] += 1
                    doc_stats[doc_id]["complete"] += 1
                elif has_metadata:
                    model_stats[model]["missing_eval"] += 1
                    doc_stats[doc_id]["incomplete"] += 1
                elif has_eval:
                    model_stats[model]["incomplete"] += 1
                    doc_stats[doc_id]["incomplete"] += 1
    
    print(f"\nğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
    for model in sorted(model_stats.keys()):
        stats = model_stats[model]
        total = stats["complete"] + stats["incomplete"] + stats["missing_eval"]
        print(f"   {model}:")
        print(f"     âœ… å®Œæ•´: {stats['complete']}")
        print(f"     âš ï¸  ç¼ºå°‘è¯„ä¼°: {stats['missing_eval']}")
        print(f"     âš ï¸  ä¸å®Œæ•´: {stats['incomplete']}")
        print(f"     æ€»è®¡: {total}")
    
    print(f"\nğŸ“„ æ–‡æ¡£ç»Ÿè®¡:")
    for doc_id in sorted(doc_stats.keys()):
        stats = doc_stats[doc_id]
        print(f"   {doc_id}: {stats['complete']} å®Œæ•´, {stats['incomplete']} ä¸å®Œæ•´")
    
    return model_stats


def check_analysis_output():
    """æ£€æŸ¥åˆ†æè¾“å‡º"""
    print("\n" + "=" * 70)
    print("3. åˆ†æè¾“å‡ºæ£€æŸ¥")
    print("=" * 70)
    
    # Try multiple possible paths
    possible_paths = [
        Path("evaluation/analysis/output"),
        Path(__file__).parent.parent / "analysis/output",
        Path(__file__).parent.parent.parent / "evaluation/analysis/output",
    ]
    
    output_dir = None
    for path in possible_paths:
        if path.exists():
            output_dir = path
            break
    
    if not output_dir or not output_dir.exists():
        print(f"âŒ åˆ†æè¾“å‡ºç›®å½•ä¸å­˜åœ¨")
        for path in possible_paths:
            print(f"   å°è¯•: {path} ({path.exists()})")
        return
    
    # æ£€æŸ¥æ‘˜è¦æ–‡ä»¶
    summary_files = list(output_dir.glob("analysis_summary*.json"))
    if summary_files:
        latest_summary = max(summary_files, key=lambda p: p.stat().st_mtime)
        print(f"\nğŸ“Š åˆ†ææ‘˜è¦: {latest_summary.name}")
        
        with open(latest_summary, 'r') as f:
            data = json.load(f)
        
        print(f"   è¿è¡Œæ•°: {data.get('n_runs', 'N/A')}")
        print(f"   æ¨¡å‹æ•°: {data.get('n_models', 'N/A')}")
        print(f"   æ–‡æ¡£æ•°: {data.get('n_documents', 'N/A')}")
        
        # æ£€æŸ¥å¯é æ€§æ‘˜è¦
        reliability = data.get("reliability_summary", {})
        completion = reliability.get("completion_rate", {})
        print(f"\n   å®Œæˆç‡:")
        for model, rate in completion.items():
            if rate == 0.0:
                print(f"     âŒ {model}: {rate}")
            elif rate < 1.0:
                print(f"     âš ï¸  {model}: {rate}")
            else:
                print(f"     âœ… {model}: {rate}")
    else:
        print("âŒ æœªæ‰¾åˆ°åˆ†ææ‘˜è¦æ–‡ä»¶")
    
    # æ£€æŸ¥å›¾è¡¨
    figures_dir = output_dir / "figures"
    if figures_dir.exists():
        figures = list(figures_dir.glob("*.png"))
        print(f"\nğŸ“ˆ å›¾è¡¨æ–‡ä»¶: {len(figures)} ä¸ª")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆç©ºæ–‡ä»¶æˆ–æŸåï¼‰
        small_files = [f for f in figures if f.stat().st_size < 1000]
        if small_files:
            print(f"   âš ï¸  å¯èƒ½æŸåçš„å›¾è¡¨ ({len(small_files)} ä¸ª):")
            for f in small_files[:5]:
                print(f"      {f.name}: {f.stat().st_size} bytes")
    else:
        print("âŒ å›¾è¡¨ç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥è¡¨æ ¼
    tables_dir = output_dir / "tables"
    if tables_dir.exists():
        tables = list(tables_dir.glob("*.csv"))
        print(f"\nğŸ“‹ è¡¨æ ¼æ–‡ä»¶: {len(tables)} ä¸ª")
        
        # æ£€æŸ¥ç©ºè¡¨æ ¼
        empty_tables = []
        for table in tables:
            try:
                with open(table, 'r') as f:
                    lines = f.readlines()
                    if len(lines) <= 1:  # åªæœ‰æ ‡é¢˜è¡Œ
                        empty_tables.append(table)
            except:
                pass
        
        if empty_tables:
            print(f"   âš ï¸  ç©ºè¡¨æ ¼ ({len(empty_tables)} ä¸ª):")
            for t in empty_tables[:5]:
                print(f"      {t.name}")
    else:
        print("âŒ è¡¨æ ¼ç›®å½•ä¸å­˜åœ¨")


def check_evaluation_results():
    """æ£€æŸ¥è¯„ä¼°ç»“æœæ–‡ä»¶çš„å†…å®¹"""
    print("\n" + "=" * 70)
    print("4. è¯„ä¼°ç»“æœæ–‡ä»¶æ£€æŸ¥")
    print("=" * 70)
    
    runs_dir = Path("evaluation/runs")
    sample_results = []
    
    for eval_file in list(runs_dir.rglob("eval_result.json"))[:5]:
        try:
            with open(eval_file, 'r') as f:
                data = json.load(f)
            
            doc_id = data.get("document_id", "unknown")
            model = eval_file.parent.parent.parent.name
            run_id = eval_file.parent.name
            
            sample_results.append({
                "model": model,
                "doc": doc_id,
                "run": run_id,
                "has_completeness": "completeness" in data,
                "has_correctness": "correctness" in data,
                "has_llm_judge": "llm_judge" in data,
            })
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è¯»å– {eval_file}: {e}")
    
    if sample_results:
        print(f"\nğŸ“‹ æ ·æœ¬è¯„ä¼°ç»“æœ (å‰5ä¸ª):")
        for result in sample_results:
            checks = []
            if result["has_completeness"]:
                checks.append("âœ… completeness")
            else:
                checks.append("âŒ completeness")
            if result["has_correctness"]:
                checks.append("âœ… correctness")
            else:
                checks.append("âŒ correctness")
            if result["has_llm_judge"]:
                checks.append("âœ… llm_judge")
            else:
                checks.append("âŒ llm_judge")
            
            print(f"   {result['model']}/{result['doc']}/{result['run']}: {', '.join(checks)}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("FAIRiAgent è¯„ä¼°è¯Šæ–­å·¥å…·")
    print("=" * 70)
    
    # æ£€æŸ¥å„ä¸ªéƒ¨åˆ†
    gt_issues = check_ground_truth()
    model_stats = check_runs_data()
    check_analysis_output()
    check_evaluation_results()
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("è¯Šæ–­æ€»ç»“")
    print("=" * 70)
    
    if model_stats:
        total_complete = sum(s["complete"] for s in model_stats.values())
        total_expected = len(model_stats) * 2 * 10  # 8 models Ã— 2 docs Ã— 10 runs
        
        print(f"\nâœ… å®Œæ•´è¿è¡Œ: {total_complete}/{total_expected}")
        if total_expected > 0:
            print(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§: {total_complete/total_expected*100:.1f}%")
        else:
            print(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§: N/A (æ— é¢„æœŸæ•°æ®)")
    else:
        print("\nâš ï¸  æœªæ‰¾åˆ°è¿è¡Œæ•°æ®")
    
    if gt_issues:
        print(f"\nâš ï¸  Ground Truth é—®é¢˜: {len(gt_issues)} ä¸ª")
        for issue in gt_issues[:5]:
            print(f"   - {issue}")
    
    print("\nğŸ’¡ å»ºè®®:")
    if total_complete < total_expected:
        missing = total_expected - total_complete
        print(f"   1. æœ‰ {missing} ä¸ªè¿è¡Œä¸å®Œæ•´ï¼Œéœ€è¦è¡¥è·‘æˆ–é‡æ–°è¯„ä¼°")
    
    if gt_issues:
        print(f"   2. ä¿®å¤ ground truth ä¸­çš„æ–‡æ¡£è·¯å¾„é—®é¢˜")
    
    print(f"   3. é‡æ–°è¿è¡Œåˆ†æ: python evaluation/analysis/run_analysis.py")


if __name__ == "__main__":
    main()

#!/bin/bash
# Run evaluation for OpenAI models in parallel
# Usage: ./run_openai_evaluation.sh [repeats] [workers]

BASE_DIR="/Users/changlinke/Documents/Main/SSB/PhD/Research/FAIRiAgent"
cd "$BASE_DIR"

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT_DIR="evaluation/runs/openai_parallel_${TIMESTAMP}"
PYTHON="/Users/changlinke/miniforge3/envs/FAIRiAgent/bin/python3"

# Default values
REPEATS=${1:-3}
WORKERS=${2:-3}

# Create output directory
mkdir -p "$OUTPUT_DIR"

echo "ğŸš€ Starting OpenAI models parallel evaluation"
echo "Output directory: $OUTPUT_DIR"
echo "Repeats: $REPEATS per document"
echo "Workers: $WORKERS parallel runs"
echo ""

# Start GPT-5 evaluation
# echo "ğŸ“Š Starting GPT-5 evaluation..."
# nohup $PYTHON evaluation/scripts/run_batch_evaluation.py \
#   --env-file evaluation/config/env.evaluation \
#   --model-configs evaluation/config/model_configs/openai_gpt5.env \
#   --ground-truth evaluation/datasets/annotated/ground_truth_v2.json \
#   --output-dir "${OUTPUT_DIR}/gpt5" \
#   --repeats $REPEATS \
#   --workers $WORKERS \
#   > "${OUTPUT_DIR}/gpt5.log" 2>&1 &
# GPT5_PID=$!
# echo "  GPT-5 PID: $GPT5_PID"

# Start GPT-4.1 evaluation
echo "ğŸ“Š Starting GPT-4.1 evaluation..."
nohup $PYTHON evaluation/scripts/run_batch_evaluation.py \
  --env-file evaluation/config/env.evaluation \
  --model-configs evaluation/config/model_configs/openai_gpt4.1.env \
  --ground-truth evaluation/datasets/annotated/ground_truth_v2.json \
  --output-dir "${OUTPUT_DIR}/gpt4.1" \
  --repeats $REPEATS \
  --workers $WORKERS \
  > "${OUTPUT_DIR}/gpt4.1.log" 2>&1 &
GPT4_1_PID=$!
echo "  GPT-4.1 PID: $GPT4_1_PID"

# Start GPT-4o evaluation
# echo "ğŸ“Š Starting GPT-4o evaluation..."
# nohup $PYTHON evaluation/scripts/run_batch_evaluation.py \
#   --env-file evaluation/config/env.evaluation \
#   --model-configs evaluation/config/model_configs/openai_gpt4o.env \
#   --ground-truth evaluation/datasets/annotated/ground_truth_v2.json \
#   --output-dir "${OUTPUT_DIR}/gpt4o" \
#   --repeats $REPEATS \
#   --workers $WORKERS \
#   > "${OUTPUT_DIR}/gpt4o.log" 2>&1 &
# GPT4O_PID=$!
# echo "  GPT-4o PID: $GPT4O_PID"

# Start O3 evaluation
echo "ğŸ“Š Starting O3 evaluation..."
nohup $PYTHON evaluation/scripts/run_batch_evaluation.py \
  --env-file evaluation/config/env.evaluation \
  --model-configs evaluation/config/model_configs/openai_o3.env \
  --ground-truth evaluation/datasets/annotated/ground_truth_v2.json \
  --output-dir "${OUTPUT_DIR}/o3" \
  --repeats $REPEATS \
  --workers $WORKERS \
  > "${OUTPUT_DIR}/o3.log" 2>&1 &
O3_PID=$!
echo "  O3 PID: $O3_PID"

echo ""
echo "âœ… All evaluations started!"
echo ""
echo "ğŸ“Š Monitor progress:"
echo "  tail -f ${OUTPUT_DIR}/*.log"
echo ""
echo "ğŸ“‹ Check status:"
echo "  ps aux | grep run_batch_evaluation"
echo ""
echo "ğŸ›‘ To stop all evaluations:"
echo "  pkill -f run_batch_evaluation"
echo ""
echo "ğŸ“ Results will be in: $OUTPUT_DIR"
echo ""
echo "ğŸ“Š After all runs complete, aggregate results:"
echo "  $PYTHON evaluation/scripts/aggregate_evaluation_results.py $OUTPUT_DIR"
echo ""
echo "   Or manually run evaluation on each model:"
echo "  $PYTHON evaluation/scripts/evaluate_outputs.py \\"
echo "    --run-dir ${OUTPUT_DIR}/gpt5 \\"
echo "    --ground-truth evaluation/datasets/annotated/ground_truth_v2.json \\"
echo "    --env-file evaluation/config/env.evaluation"

